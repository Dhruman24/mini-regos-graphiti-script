# Neo4j Configuration (Required)
NEO4J_URI=neo4j+s://your-instance.databases.neo4j.io
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-password-here

# OpenAI/LLM Configuration (Required by graphiti-core)
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1

# API Security
GRAPHITI_API_KEY=your-secure-api-key-here

# Performance Tuning
# Maximum number of concurrent episode processing (default: 3)
# Lower values reduce load on Neo4j and LLM API, higher values increase throughput
# Recommended: 1-3 for cloud Neo4j, 3-5 for local/dedicated instances
MAX_INFLIGHT_EPISODES=1

# Maximum chunks accepted per request (default: 25)
# Must match or exceed N8N batch size
MAX_CHUNKS_PER_REQUEST=25

# Timeout for each episode processing in seconds (default: 90, minimum: 10)
# Increase if you have large chunks or slow LLM responses
EPISODE_TIMEOUT_SECONDS=90

# Token Usage Optimization
# Enable deduplication to skip already processed chunks (default: true)
# This can save significant tokens if you re-run pipelines
ENABLE_DEDUPLICATION=true

# Database Schema
# Run schema setup (indices and constraints) on startup (default: false)
# Set to true only on first run or after schema changes
RUN_SCHEMA=false

# Logging
# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
LOG_LEVEL=INFO

# Server Configuration
# Number of Uvicorn workers (default: 1)
# Keep at 1 to maintain in-memory deduplication cache across requests
WEB_CONCURRENCY=1
